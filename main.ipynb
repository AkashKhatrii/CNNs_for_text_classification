{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# import torchtext as text\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "# from torchtext.vocab import vocab, GloVe\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertModel, BertTokenizer, AutoTokenizer\n",
    "from datasets import load_dataset, load_dataset_builder, Dataset\n",
    "torch.manual_seed(42)\n",
    "from sklearn.metrics import f1_score\n",
    "# glove = GloVe(name='840B', dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 766,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 768,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(reviews):\n",
    "    max_len = 0 # we need to defin the maximum length of a review in our dataset because cnns take fixed sized inputs\n",
    "    tokenized_reviews = []\n",
    "\n",
    "    # idx = 2\n",
    "    for review in reviews:\n",
    "        tokenized = word_tokenize(review)\n",
    "        tokenized_reviews.append(tokenized)\n",
    "        max_len = max(max_len, len(tokenized))\n",
    "    \n",
    "    return tokenized_reviews, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokens to their idx and padding the reviews\n",
    "def convertidx(tokenized_reviews, max_len, word2ix):\n",
    "    input_ids = []\n",
    "    for review in tokenized_reviews:\n",
    "        review += ['<PAD>'] * (max_len - len(review))\n",
    "        input_id = [word2ix.get(token, word2ix['<UNK>']) for token in review]\n",
    "        input_ids.append(input_id)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training and collecting results (because ran code with option 'Run all cells')\n",
    "DATASET = 'SST'\n",
    "f = open(\"results.txt\", \"a\")\n",
    "f.write(f\"\\n{DATASET}\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_SIZES = [2, 3, 5]\n",
    "NO_OF_FILTERS = [200, 200, 400]\n",
    "EMB = 'F'\n",
    "FREEZE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"results.txt\", \"a\")\n",
    "f.write(f\"\\n\\nEmbeddings: {EMB}\")\n",
    "f.write(f\"\\nFilter sizes: {FILTER_SIZES}\")\n",
    "f.write(f\"\\nNo of filters: {NO_OF_FILTERS}\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive count:  5331\n",
      "Negative count:  5331\n",
      "(10662,) (10662,)\n"
     ]
    }
   ],
   "source": [
    "pos_arr = []\n",
    "neg_arr = []\n",
    "\n",
    "with open(r'rt-polaritydata\\rt-polarity.pos', 'rb') as f:\n",
    "    pos_count = 0\n",
    "    for line in f:\n",
    "        pos_count += 1\n",
    "        pos_arr.append(line.decode(errors='ignore').lower().strip()) # strings are in base64 format, therefore need to decode it \n",
    "    print(\"Positive count: \", pos_count)\n",
    "\n",
    "with open(r'rt-polaritydata\\rt-polarity.neg', 'rb') as f:\n",
    "    neg_count = 0\n",
    "    for line in f:\n",
    "        neg_count += 1\n",
    "        neg_arr.append(line.decode(errors='ignore').lower().strip()) # strings are in base64 format, therefore need to decode it \n",
    "    print(\"Negative count: \", neg_count)\n",
    "\n",
    "reviews = neg_arr.copy()\n",
    "reviews.extend(pos_arr)\n",
    "labels = [0] * len(neg_arr)\n",
    "labels.extend([1] * len(pos_arr))\n",
    "\n",
    "reviews = np.array(reviews)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(reviews.shape, labels.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_mr_sentences, test_mr_sentences, train_mr_labels, test_mr_labels = train_test_split(reviews, labels, test_size=0.15, random_state=20)\n",
    "\n",
    "train_mr_sentences, val_mr_sentences, train_mr_labels, val_mr_labels = train_test_split(train_mr_sentences, train_mr_labels, test_size=0.20, random_state= 8)\n",
    "\n",
    "tokenized_mr_train, max_mr_train_len = tokenize(train_mr_sentences)\n",
    "tokenized_mr_test, max_mr_test_len = tokenize(test_mr_sentences)\n",
    "tokenized_mr_val, max_mr_val_len = tokenize(val_mr_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2ix for MR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16689\n"
     ]
    }
   ],
   "source": [
    "if DATASET == 'MR':\n",
    "    mr_word2ix = {'<PAD>': 0, '<UNK>': 1}\n",
    "    mr_ix2word = {0: '<PAD>', 1: '<UNK>'}\n",
    "    idx = 2\n",
    "    count = 0\n",
    "    for review in tokenized_mr_train:\n",
    "        count += 1\n",
    "        for token in review:\n",
    "            if token not in mr_word2ix:\n",
    "                mr_word2ix[token] = idx\n",
    "                mr_ix2word[idx] = token\n",
    "                idx += 1\n",
    "    print(len(mr_word2ix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MR input to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'MR':\n",
    "    mr_train_input_ids = convertidx(tokenized_mr_train, max_mr_train_len, mr_word2ix)\n",
    "    mr_test_input_ids = convertidx(tokenized_mr_test, max_mr_test_len, mr_word2ix)\n",
    "    mr_val_input_ids = convertidx(tokenized_mr_val, max_mr_val_len, mr_word2ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 6920\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "if DATASET == 'SST':\n",
    "    dataset = load_dataset(\"gpt3mix/sst2\")\n",
    "    print(dataset)\n",
    "\n",
    "    sst_train = dataset['train']\n",
    "    sst_test = dataset['test']\n",
    "    sst_val = dataset['validation']\n",
    "\n",
    "    train_sentences = sst_train['text']\n",
    "    train_labels = sst_train['label']\n",
    "    test_sentences = sst_test['text']\n",
    "    test_labels = sst_test['label']\n",
    "    val_sentences = sst_val['text']\n",
    "    val_labels = sst_val['label']\n",
    "\n",
    "    tokenized_sst_train, max_sst_train_len = tokenize(train_sentences)\n",
    "    tokenized_sst_test, max_sst_test_len = tokenize(test_sentences)\n",
    "    tokenized_sst_val, max_sst_val_len = tokenize(val_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2ix for SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16274\n"
     ]
    }
   ],
   "source": [
    "if DATASET == 'SST':\n",
    "    sst_word2ix = {'<PAD>': 0, '<UNK>': 1}\n",
    "    sst_ix2word = {0: '<PAD>', 1: '<UNK>'}\n",
    "    idx = 2\n",
    "    count = 0\n",
    "    for sentence in tokenized_sst_train:\n",
    "        count += 1\n",
    "        for token in sentence:\n",
    "            if token not in sst_word2ix:\n",
    "                sst_word2ix[token] = idx\n",
    "                sst_ix2word[idx] = token\n",
    "                idx += 1\n",
    "    print(len(sst_word2ix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SST input to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'SST':\n",
    "    sst_train_input_ids = convertidx(tokenized_sst_train, max_sst_train_len, sst_word2ix)\n",
    "    sst_test_input_ids = convertidx(tokenized_sst_test, max_sst_test_len, sst_word2ix)\n",
    "    sst_val_input_ids = convertidx(tokenized_sst_val, max_sst_val_len, sst_word2ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subj Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "subj_dataset = load_dataset(\"SetFit/subj\")\n",
    "\n",
    "subj_train = subj_dataset['train']\n",
    "subj_test = subj_dataset['test'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_train_text = subj_train['text']\n",
    "subj_train_label = subj_train['label']\n",
    "subj_test_text = subj_test['text']\n",
    "subj_test_label = subj_test['label']\n",
    "\n",
    "# creating val set\n",
    "from sklearn.model_selection import train_test_split\n",
    "subj_train_text, subj_val_text, subj_train_label, subj_val_label = train_test_split(subj_train_text, subj_train_label, test_size=0.20, random_state=20)\n",
    "\n",
    "tokenized_subj_train, max_subj_train_len = tokenize(subj_train_text)\n",
    "tokenized_subj_test, max_subj_test_len = tokenize(subj_test_text)\n",
    "tokenized_subj_val, max_subj_val_len = tokenize(subj_val_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2ix for Subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18013\n"
     ]
    }
   ],
   "source": [
    "if DATASET == 'SUBJ':\n",
    "    subj_word2ix = {'<PAD>': 0, '<UNK>': 1}\n",
    "    subj_ix2word = {0: '<PAD>', 1: '<UNK>'}\n",
    "    idx = 2\n",
    "    count = 0\n",
    "    for sentence in tokenized_subj_train:\n",
    "        count += 1\n",
    "        for token in sentence:\n",
    "            if token not in subj_word2ix:\n",
    "                subj_word2ix[token] = idx\n",
    "                subj_ix2word[idx] = token\n",
    "                idx += 1\n",
    "    print(len(subj_word2ix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subj input to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'SUBJ':\n",
    "    subj_train_input_ids = convertidx(tokenized_subj_train, max_subj_train_len, subj_word2ix)\n",
    "    subj_test_input_ids = convertidx(tokenized_subj_test, max_subj_test_len, subj_word2ix)\n",
    "    subj_val_input_ids = convertidx(tokenized_subj_val, max_subj_val_len, subj_word2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_path = r\"C:\\Users\\akash\\Downloads\\crawl-300d-2M\\crawl-300d-2M.vec\"\n",
    "gloVe_path = r\".vector_cache\\glove.840B.300d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating embedding list (GloVe, fastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an embedding dictionary which stores the embeddings for the words in our vocabulary (to feed it to emdedding layer), we could use embeddings directly but this was the recommendedway in many articles\n",
    "def get_embeddings(emb, word2ix):\n",
    "    embeddings = np.random.uniform(-0.25, 0.25, (len(word2ix), 300))\n",
    "    embeddings[word2ix['<PAD>']] = np.zeros((300,))\n",
    "\n",
    "    if emb == 'glove':\n",
    "        fin = open(gloVe_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    else:\n",
    "        fin = open(fastText_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    count = 0\n",
    "    for line in tqdm(fin):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        word = tokens[0]\n",
    "        if word in word2ix:\n",
    "            count += 1\n",
    "            embeddings[word2ix[word]] = np.array(tokens[1:], dtype=np.float32)\n",
    "    embeddings = torch.tensor(embeddings)\n",
    "    print(len(word2ix), count)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating embeddings list (Bert) Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'MR':\n",
    "    mr_glove_embeddings = get_embeddings(\"glove\", mr_word2ix)\n",
    "    mr_fast_embeddings = get_embeddings(\"fastText\", mr_word2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [00:20, 105913.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16274 15657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1999996it [00:18, 110739.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16274 15669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if DATASET == 'SST':\n",
    "    sst_glove_embeddings = get_embeddings(\"glove\", sst_word2ix)\n",
    "    sst_fast_embeddings = get_embeddings(\"fastText\", sst_word2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [00:20, 105761.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18013 16796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1999996it [00:18, 110652.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18013 16690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if DATASET == 'SUBJ':\n",
    "    subj_glove_embeddings = get_embeddings(\"glove\", subj_word2ix)\n",
    "    subj_fast_embeddings = get_embeddings(\"fastText\", subj_word2ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import (TensorDataset, DataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For MR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'MR':\n",
    "    train_X = torch.tensor(mr_train_input_ids)\n",
    "    train_y = torch.tensor(train_mr_labels)\n",
    "    val_X = torch.tensor(mr_val_input_ids)\n",
    "    val_y = torch.tensor(val_mr_labels)\n",
    "    test_X = torch.tensor(mr_test_input_ids)\n",
    "    test_y = torch.tensor(test_mr_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For SST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'SST':\n",
    "    train_X = torch.tensor(sst_train_input_ids)\n",
    "    train_y = torch.tensor(train_labels)\n",
    "    val_X = torch.tensor(sst_val_input_ids)\n",
    "    val_y = torch.tensor(val_labels)\n",
    "    test_X = torch.tensor(sst_test_input_ids)\n",
    "    test_y = torch.tensor(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Subj dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'SUBJ':\n",
    "    train_X = torch.tensor(subj_train_input_ids)\n",
    "    train_y = torch.tensor(subj_train_label)\n",
    "    val_X = torch.tensor(subj_val_input_ids)\n",
    "    val_y = torch.tensor(subj_val_label)\n",
    "    test_X = torch.tensor(subj_test_input_ids)\n",
    "    test_y = torch.tensor(subj_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_X, train_y)\n",
    "train_dataLoader = DataLoader(train_data, batch_size=50, shuffle=True)\n",
    "test_data = TensorDataset(test_X, test_y)\n",
    "test_dataLoader = DataLoader(test_data, batch_size=50, shuffle=True)\n",
    "val_data = TensorDataset(val_X, val_y)\n",
    "val_dataLoader = DataLoader(val_data, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, pretrained_embedding=None, freeze_embedding=False, vocab_size=None, embed_dim=300, filter_sizes=[3, 4, 5], num_filters=[100, 100, 100], num_classes=2, dropout=0.5):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        if pretrained_embedding is not None:\n",
    "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding, freeze=freeze_embedding)\n",
    "            # print(self.embedding.shape)\n",
    "        else:\n",
    "            self.embed_dim = embed_dim\n",
    "            self.embedding = nn.Embedding(num_embeddings=vocab_size,embedding_dim=self.embed_dim,padding_idx=0,\n",
    "                                          )\n",
    "        # conv1d because we will convolve in only 1 direction, row wise, just as ngrams\n",
    "        self.conv1d = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=self.embed_dim,\n",
    "                      out_channels= num_filters[i], # number of feature maps\n",
    "                      kernel_size=filter_sizes[i]) # filter size\n",
    "                      for i in range(len(filter_sizes))\n",
    "        ])\n",
    "\n",
    "        self.linear = nn.Linear(np.sum(num_filters), num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x_emb = self.embedding(input_ids).float() # (batch_size, max_len, embed_dim)\n",
    "        # print(\"emb_input: \", x_emb.shape)\n",
    "\n",
    "        # Conv1d takes word embedding dimensions as input (in_channels)\n",
    "        x_emb = x_emb.permute(0, 2, 1) # (batch_size, embed_dim, max_len)\n",
    "        # print(\"reshaped_emb_input: \", x_emb.shape)\n",
    "\n",
    "        # x_conv_list = [F.relu(conv1d(x_emb)) for conv1d in self.conv1d]\n",
    "\n",
    "        x_conv_list = []\n",
    "        '''\n",
    "        contains feature maps for all the sentences in the batch\n",
    "        Therefore, for every sentence, there will be 100 feature maps of a certain filter size\n",
    "        suppose, 50 sentence, the output will be (supposing using filter size 3 and total filters = 3),\n",
    "        (50, 100, 60) where 60 is the dimension of feature map (after performing convolution operation)\n",
    "        \n",
    "        '''\n",
    "        for layer in self.conv1d: # self.conv1d will have no of layers equal to the filter sizes, i.e if f_sizes = [3, 4, 5], then no of layers = 3\n",
    "            out = layer(x_emb) # convolving the sentences with a certain filter size\n",
    "            # print(out.shape) (batch_size, num_filters, output of that filter size after convolution)\n",
    "            out = F.relu(out) # applying relu on the output\n",
    "            # print(out.shape)\n",
    "            x_conv_list.append(out)\n",
    "            \n",
    "        # print(\"length of conv list: \",len(x_conv_list))\n",
    "        # print(len(x_conv_list[0]))\n",
    "        # print(x_conv_list[0].shape)\n",
    "        # print(x_conv_list[1].shape)\n",
    "        # print(x_conv_list[2].shape)\n",
    "        # print(x_conv_list[3].shape)\n",
    "        \n",
    "\n",
    "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
    "            for x_conv in x_conv_list]\n",
    "\n",
    "        \n",
    "        # print(\"length of pool list: \", len(x_pool_list))\n",
    "        # print(x_pool_list[0])\n",
    "        # Concatenate x_pool_list to feed the fully connected layer.\n",
    "        # Output shape: (b, sum(num_filters))\n",
    "        x_fc_input = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
    "                         dim=1)\n",
    "        \n",
    "        \n",
    "        # Compute logits. Output shape: (batch_size, num_classes)\n",
    "        logits = self.linear(self.dropout(x_fc_input))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREEZE = True # manually run this cell to train the model without finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'MR':\n",
    "    if EMB == 'G':\n",
    "        embeddings = mr_glove_embeddings\n",
    "    else:\n",
    "        embeddings = mr_fast_embeddings\n",
    "    word2ix = mr_word2ix\n",
    "elif DATASET == 'SST':\n",
    "    if EMB == 'G':\n",
    "        embeddings = sst_glove_embeddings\n",
    "    else:\n",
    "        embeddings = sst_fast_embeddings\n",
    "    word2ix = sst_word2ix\n",
    "else:\n",
    "    if EMB == 'G':\n",
    "        embeddings = subj_glove_embeddings\n",
    "    else:\n",
    "        embeddings = subj_fast_embeddings\n",
    "    word2ix = subj_word2ix\n",
    "\n",
    "cnn_model = CNN(pretrained_embedding=None,\n",
    "                        freeze_embedding=FREEZE,\n",
    "                        vocab_size=len(word2ix),\n",
    "                        embed_dim=300,\n",
    "                        filter_sizes=FILTER_SIZES,\n",
    "                        num_filters=NO_OF_FILTERS,\n",
    "                        num_classes=2,\n",
    "                        dropout=0.6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LEARNING RATE: 0.1\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 89.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.7625382992861082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 358.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.6374097433355119\n",
      "Val accuracy:  64.20202020202021\n",
      "Val F1 accuracy:  60.872814804239056\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 100.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.637005295256059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 360.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.6002695129977332\n",
      "Val accuracy:  69.53535353535354\n",
      "Val F1 accuracy:  67.7404934880622\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 103.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.5509159957333435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 364.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.5916820598973168\n",
      "Val accuracy:  68.14141414141415\n",
      "Val F1 accuracy:  66.14223636367508\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 103.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.48009997606277466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 364.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.5681101630131403\n",
      "Val accuracy:  69.69696969696969\n",
      "Val F1 accuracy:  69.13549661589525\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 103.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.41523735879136503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 363.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.5698237849606408\n",
      "Val accuracy:  70.4040404040404\n",
      "Val F1 accuracy:  68.61849705421798\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 103.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.3591656040587871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 403.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.5718346155352063\n",
      "Val accuracy:  71.54545454545455\n",
      "Val F1 accuracy:  70.87238353990014\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 103.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.3078277327602716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 403.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.5472906397448646\n",
      "Val accuracy:  71.86868686868688\n",
      "Val F1 accuracy:  71.41536618653605\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 102.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.266539671009393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 409.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.5457764400376214\n",
      "Val accuracy:  72.7878787878788\n",
      "Val F1 accuracy:  72.20013430078632\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 103.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.22600880720846944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 358.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.5950714631213082\n",
      "Val accuracy:  70.04040404040404\n",
      "Val F1 accuracy:  68.70320216775745\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 104.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.1951811856074299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 408.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.5711164044009315\n",
      "Val accuracy:  73.30303030303031\n",
      "Val F1 accuracy:  72.8160223279687\n",
      "\n",
      "LEARNING RATE: 0.15\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 105.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.19349815197985806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 408.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.6865217222107781\n",
      "Val accuracy:  68.33333333333333\n",
      "Val F1 accuracy:  66.43000605290183\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 105.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.183614582895375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 412.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.6277393615908093\n",
      "Val accuracy:  72.1010101010101\n",
      "Val F1 accuracy:  71.37254236151225\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 104.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.15465222071293447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 386.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.631396492322286\n",
      "Val accuracy:  71.39393939393939\n",
      "Val F1 accuracy:  70.75141352365534\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 105.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.15025160161496923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 410.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.6197957371671995\n",
      "Val accuracy:  70.85858585858585\n",
      "Val F1 accuracy:  70.50910833054888\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 105.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.12178701872555472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 356.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.7425930549701055\n",
      "Val accuracy:  70.73737373737373\n",
      "Val F1 accuracy:  69.22792176443534\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 104.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.11759113228256754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 407.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.6765238493680954\n",
      "Val accuracy:  72.0909090909091\n",
      "Val F1 accuracy:  71.6283318528404\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 105.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.09616686435912153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 407.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.6548079401254654\n",
      "Val accuracy:  71.26262626262627\n",
      "Val F1 accuracy:  70.70630315112183\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 105.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.08349720490493363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 402.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.6665197511514028\n",
      "Val accuracy:  71.70707070707071\n",
      "Val F1 accuracy:  71.41279112602456\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 104.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.07318937949997058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 430.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.7154515948560503\n",
      "Val accuracy:  70.92929292929293\n",
      "Val F1 accuracy:  70.22060740390158\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 104.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.07076273267592886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 363.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.759763091802597\n",
      "Val accuracy:  72.72727272727272\n",
      "Val F1 accuracy:  72.37334142894808\n",
      "\n",
      "LEARNING RATE: 0.2\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 104.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.07708192226751674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 404.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.7180887129571702\n",
      "Val accuracy:  72.48484848484848\n",
      "Val F1 accuracy:  71.82377269238795\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 104.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.07662307976899173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 350.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.7700362337960137\n",
      "Val accuracy:  72.29292929292929\n",
      "Val F1 accuracy:  71.75188546906054\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 104.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.08334481114481422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 402.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.7459264662530687\n",
      "Val accuracy:  73.32323232323232\n",
      "Val F1 accuracy:  72.88826473016135\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 104.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.07122433878709193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 418.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.8020736243989732\n",
      "Val accuracy:  72.15151515151516\n",
      "Val F1 accuracy:  71.74314706714921\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 104.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.0756750688452622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 355.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.7772238320774503\n",
      "Val accuracy:  72.23232323232324\n",
      "Val F1 accuracy:  71.86600645645365\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 105.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.06837263066495816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 406.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.8356140951315562\n",
      "Val accuracy:  73.07070707070707\n",
      "Val F1 accuracy:  72.39860301132836\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 104.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.07298124550419638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 396.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.8782962411642075\n",
      "Val accuracy:  71.84848484848484\n",
      "Val F1 accuracy:  71.23608217897895\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 105.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.07064691850607344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 409.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.8279485636287265\n",
      "Val accuracy:  71.56565656565657\n",
      "Val F1 accuracy:  71.12721444295205\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 105.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.06350174850764141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 403.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.9040196273061964\n",
      "Val accuracy:  71.83838383838383\n",
      "Val F1 accuracy:  71.26954320248194\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 105.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.06637013650988396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 401.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.8537693371375402\n",
      "Val accuracy:  72.37373737373737\n",
      "Val F1 accuracy:  71.95967088102755\n",
      "\n",
      "Maximum val accuracy of f1: 72.88826473016135 was achieved with learning rate 0.2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_model.train()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "learning_rates = [0.1, 0.15, 0.2]\n",
    "max_accuracy = -1\n",
    "max_accuracy_lr = -1\n",
    "for learning_rate in learning_rates:\n",
    "    print(f\"\\nLEARNING RATE:\", learning_rate)\n",
    "    optimizer = optim.Adadelta(cnn_model.parameters(),lr=learning_rate,rho=0.95)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        train_loss = []\n",
    "        cnn_model.train()\n",
    "        for reviews, labels in tqdm(train_dataLoader):\n",
    "                cnn_model.zero_grad()\n",
    "                outputs = cnn_model(reviews.to(device))\n",
    "                labels = labels.type(torch.LongTensor).to(device)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        avg_train_loss = np.mean(train_loss)\n",
    "        print(\"Train loss: \", avg_train_loss)\n",
    "\n",
    "        cnn_model.eval()\n",
    "        val_loss = []\n",
    "        val_accuracy = []\n",
    "        val_f1 = []\n",
    "        for reviews, labels in tqdm(val_dataLoader):\n",
    "            with torch.no_grad():\n",
    "                outputs = cnn_model(reviews.to(device))\n",
    "            labels = labels.type(torch.LongTensor).to(device)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            # Get the predictions\n",
    "            preds = torch.argmax(outputs, dim=1).flatten()\n",
    "\n",
    "            # Calculate the accuracy rate\n",
    "            accuracy = (preds == labels).cpu().numpy().mean() * 100\n",
    "            f1 = f1_score(labels.cpu(), preds.cpu(), average='macro').mean() * 100\n",
    "            val_accuracy.append(accuracy)\n",
    "            val_f1.append(f1)\n",
    "\n",
    "        # Compute the average accuracy and loss over the validation set.\n",
    "        val_loss = np.mean(val_loss)\n",
    "        val_accuracy = np.mean(val_accuracy)\n",
    "        val_f1 = np.mean(val_f1)\n",
    "\n",
    "        if val_f1 > max_accuracy:\n",
    "             max_accuracy = val_f1\n",
    "             max_accuracy_lr = learning_rate\n",
    "\n",
    "             torch.save({\n",
    "            \"model_param\":cnn_model.state_dict(),\n",
    "            \"optim_param\": optimizer.state_dict(),\n",
    "            \"lr\": learning_rate,\n",
    "            \"acc\":max_accuracy,\n",
    "            }, f\"saved_model/model\")\n",
    "\n",
    "        print(\"Val loss: \", val_loss)\n",
    "        print(\"Val accuracy: \", val_accuracy)\n",
    "        print(\"Val F1 accuracy: \", val_f1)\n",
    "\n",
    "print(f\"\\nMaximum val accuracy of f1: {max_accuracy} was achieved with learning rate {max_accuracy_lr}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model learning rate: 0.2 and accuracy on val set: 72.88826473016135\n"
     ]
    }
   ],
   "source": [
    "model_path =  r\"saved_model\\model\"\n",
    "checkpoint = torch.load(model_path)\n",
    "best_model = CNN(pretrained_embedding=None,\n",
    "                        freeze_embedding=FREEZE,\n",
    "                        vocab_size=len(word2ix),\n",
    "                        embed_dim=300,\n",
    "                        filter_sizes=FILTER_SIZES,\n",
    "                        num_filters=NO_OF_FILTERS,\n",
    "                        num_classes=2,\n",
    "                        dropout=0.5).to(device)\n",
    "optimizer = optim.Adadelta(cnn_model.parameters(),lr=checkpoint['lr'],rho=0.95)\n",
    "best_model.load_state_dict(checkpoint[\"model_param\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optim_param\"])\n",
    "print(f\"\"\"Best model learning rate: {checkpoint[\"lr\"]} and accuracy on val set: {checkpoint[\"acc\"]}\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:00<00:00, 99.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set:  73.93822393822394\n",
      "F1 accuracy on test set:  73.41054700518366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_model.eval()\n",
    "test_accuracy = []\n",
    "test_f1 = []\n",
    "for reviews, labels in tqdm(test_dataLoader):\n",
    "    with torch.no_grad():\n",
    "        outputs = best_model(reviews.to(device))\n",
    "        labels = labels.type(torch.LongTensor).to(device)\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(outputs, dim=1).flatten()\n",
    "        accuracy = (preds == labels).cpu().numpy().mean() * 100\n",
    "        f1 = f1_score(labels.cpu(), preds.cpu(), average='macro').mean() * 100\n",
    "        test_accuracy.append(accuracy)\n",
    "        test_f1.append(f1)\n",
    "\n",
    "test_accuracy = np.mean(test_accuracy)\n",
    "test_f1 = np.mean(test_f1)\n",
    "\n",
    "print(\"Accuracy on test set: \", test_accuracy)\n",
    "print(\"F1 accuracy on test set: \", test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings  F\n",
      "Finetuned:  False\n",
      "Filter sizes:  [2, 3, 5]\n",
      "No of filters:  [200, 200, 400]\n"
     ]
    }
   ],
   "source": [
    "print(\"Embeddings \", EMB)\n",
    "print(\"Finetuned: \", not FREEZE)\n",
    "print(\"Filter sizes: \", FILTER_SIZES)\n",
    "print(\"No of filters: \", NO_OF_FILTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"results.txt\", \"a\")\n",
    "f.write(f\"\\n\\nFinetuned: {not FREEZE}\")\n",
    "f.write(f\"\\nTest accuracy: {test_accuracy}\")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = torch.sum(train_y) # labels are 0 and 1 so if sum is greater than half the length of the training set, then majority label is 1\n",
    "majority_label = 0\n",
    "if sum > (train_y.shape[0] / 2):\n",
    "    majority_label = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 894,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majority_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_label_test_y = torch.full((test_y.shape[0],), majority_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1821])"
      ]
     },
     "execution_count": 896,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majority_label_test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_dataset = TensorDataset(test_X, majority_label_test_y)\n",
    "baseline_dataLoader = DataLoader(baseline_dataset, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "baseline_accuracy = []\n",
    "baseline_f1 = []\n",
    "for reviews, labels in tqdm(baseline_dataLoader):\n",
    "    with torch.no_grad():\n",
    "        outputs = best_model(reviews.to(device))\n",
    "        labels = labels.type(torch.LongTensor).to(device)\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(outputs, dim=1).flatten()\n",
    "        accuracy = (preds == labels).cpu().numpy().mean() * 100\n",
    "        f1 = f1_score(labels.cpu(), preds.cpu(), average='macro').mean() * 100\n",
    "        baseline_accuracy.append(accuracy)\n",
    "        baseline_f1.append(f1)\n",
    "\n",
    "baseline_accuracy = np.mean(baseline_accuracy)\n",
    "baseline_f1 = np.mean(baseline_f1)\n",
    "\n",
    "print(\"Baseline accuracy: \", baseline_accuracy)\n",
    "print(\"Baseline f1 acccuracy: \", baseline_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"results.txt\", \"a\")\n",
    "f.write(f\"\\n\\nBaseline accuracy: {baseline_accuracy}\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Regrettably, this film, despite a promising premise and commendable performances, flounders in execution. Its narrative lacks coherence, characters lack depth, and thematic resonance is absent. Visual flair and a talented cast can't salvage this disjointed endeavor, leaving audiences longing for substance and cohesion in a cinematic experience.\"\n",
    "# tokens = word_tokenize(text.lower())\n",
    "# padded_tokens = tokens + ['<PAD>'] * (max_mr_train_len - len(tokens))\n",
    "# input_id = [mr_word2ix.get(token, mr_word2ix['<UNK>']) for token in padded_tokens]\n",
    "# input_id = torch.tensor(input_id)\n",
    "# print(input_id.shape)\n",
    "# input_id = input_id.unsqueeze(dim=0)\n",
    "# print(input_id.shape)\n",
    "# # Compute logits\n",
    "# with torch.no_grad():\n",
    "#     logits = best_model.forward(input_id.to(device))\n",
    "# print(logits.shape)\n",
    "# print(logits)\n",
    "\n",
    "#     #  Compute probability\n",
    "# probs = F.softmax(logits, dim=1)\n",
    "# print(probs.shape)\n",
    "# print(probs)\n",
    "# probs = probs.squeeze(dim=0)\n",
    "# print(probs.shape)\n",
    "# print(probs)\n",
    "\n",
    "# if probs[1] > 0.5:\n",
    "#     print(f\"This review is positive.\")\n",
    "# else:\n",
    "#     print(f\"This review is negative.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
